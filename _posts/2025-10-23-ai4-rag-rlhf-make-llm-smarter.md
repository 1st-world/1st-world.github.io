---
title: "AI는 어떻게 언어를 이해하는가? (4) - LLM을 더 똑똑하게 만드는 RAG & RLHF"
author: 1st-world
date: 2025-10-23 23:20:00 +0900
last_modified_at: 2025-10-26 23:35:00 +0900
categories: [Artificial Intelligence, Machine Learning]
tags: [nlp, llm, rag, rl, rlhf, rlaif]
pin: false
---

지난 [3부 - 거대 언어 모델(LLM)의 등장](/posts/ai3-the-emergence-of-llm/)에서는 트랜스포머 아키텍처와 스케일링 법칙이 만나 ‘**거대 언어 모델(LLM, Large Language Model)**’이 탄생하는 과정을 살펴보았습니다. 하지만 우리는 동시에 이 강력한 LLM이 가진 한계점도 확인했습니다.

* **환각(Hallucination):** 그럴듯한 거짓말을 자신 있게 생성합니다.

* **데이터 병목(Data Bottleneck):** 데이터가 부족하면 모델을 키워도 성능 향상을 기대할 수 없습니다.

* **최신성 부족(Knowledge Cutoff):** 해당 모델의 학습이 끝난 뒤에 일어난 일은 알지 못합니다.

이러한 문제들은 모델을 더 크게 만든다고(Scaling) 해결되지 않습니다. 가성비는 갈수록 크게 떨어지고, 공급해야 할 데이터도 고갈되고 있죠. 환각과 같은 근본적인 문제를 해결할 수도 없습니다.

그래서 연구자들은 스케일링 외에 LLM을 보완할 수 있는 기술을 고안했습니다. 바로 **RAG**와 **RLHF**입니다.

---

## 1. RAG: LLM에게 '오픈북'을 허용하다

LLM의 환각과 최신성 부족 등의 문제는 근본적으로 '학습된 지식'에만 의존하기 때문에 발생합니다. 3년 전에 외운 교과서 내용만으로 오늘 시험을 보는 것과 같습니다.

**RAG(R**etrieval-**A**ugmented **G**eneration, **검색 증강 생성)** 기술은 이 문제를 영리하게 해결합니다.

> **비유: 오픈북 시험 📖**
>
> * **기본 LLM:** '클로즈드(Closed)북 시험'입니다. 오직 자신이 암기한 것(파라미터)에만 의존해 답을 씁니다. 그래서 최신 정보를 알지 못하고, 기억이 희미하면 종종 지어냅니다.
>
> * **RAG 적용:** '오픈북 시험'입니다. 질문을 받으면 먼저 책(데이터베이스, 인터넷)을 검색(**R**etrieval)하여 관련 내용을 찾은 뒤, 그 내용을 참고(**A**ugmented)하여 답을 생성(**G**eneration)합니다.
{: .prompt-tip }

### RAG는 어떻게 동작할까요?

사용자가 질문을 하면, RAG 시스템은 LLM에게 바로 질문을 넘기지 않습니다.

1. **검색(Retrieve)**
    * 사용자의 질문(예: "어제 A회사 주가 어땠어?")을 1, 2부에서 배운 **임베딩(Embedding)** 기술로 벡터로 변환합니다.
    * 이 질문 벡터와 가장 의미적으로 유사한 문서를 미리 구축해 둔 **벡터 데이터베이스(Vector DB)**(예: FAISS, Chroma, Milvus)나 **사전 색인된 문서 저장소**에서 찾아옵니다.

    * 검색 결과 예시
        ```
        …, A회사 주가는 어제 10% 상승 마감, …
        ```

2. **증강(Augment)**
    * 찾아온 '참고 자료'와 '원본 질문'을 하나의 '특별한 프롬프트'로 재조립합니다.

    * 프롬프트 예시
        ```
        다음 [참고 자료]를 바탕으로 [질문]에 답하세요. 자료에 없는 내용은 지어내지 마세요.
        [참고 자료]: A회사 주가는 어제 10% 상승 마감
        [질문]: 어제 A회사 주가 어땠어?
        ```

3. **생성(Generate)**
    * LLM은 이 '증강된 프롬프트'를 받아 답변을 생성합니다.

    * 답변 예시
        ```
        어제 A회사의 주가는 10% 상승 마감했습니다.
        ```

> **RAG의 지식 창고: Vector DB vs. 실시간 웹 검색**
>
> 본문에서 언급한 '벡터 데이터베이스(Vector DB)' 방식은 기업의 내부 문서나 위키피디아처럼 미리 정해진 범위의 지식을 다룰 때 가장 널리 쓰이는 표준 RAG 방식입니다.
>
> 하지만 "어제 A회사 주가"나 "오늘 날씨" 같은 실시간(Real-time) 정보가 필요한 경우, 이 '검색(Retrieve)' 단계를 Google 검색 API 등 **실제 웹 검색 엔진**으로 대체하거나 함께 사용할 수 있습니다. 이를 **실시간 웹 RAG**라고 부르며, 이는 LLM이 정적 DB를 넘어 최신 인터넷 정보까지 접근하게 해주는 강력한 확장입니다.
{: .prompt-tip }

### RAG의 효과

* **최신성 확보:** 외부 벡터 DB를 최신으로 유지하거나(표준 RAG), 웹 검색을 사용하면(실시간 웹 RAG) LLM을 재학습시키지 않고도 최신 정보를 바탕으로 답할 수 있습니다.

* **환각 억제:** '참고 자료'라는 명확한 근거를 바탕으로 답하게 하므로(Grounding), 거짓말을 할 확률을 기존 대비 크게 줄일 수 있습니다. (예: "Retrieved Source: ○○○")

* **도메인 특화 가능:** 기업 내부망에서만 접근 가능한 문서(사내 문서, 고객 데이터, 내부 매뉴얼 등)를 '지식 창고'로 활용하면 내부 정보를 기반으로 답하는 '사내 챗봇'을 만들 수 있습니다.

---

## 2. RLHF: LLM을 '안전하게 길들이기'

RAG가 LLM의 '지식' 문제를 해결했다면, **RLHF(R**einforcement **L**earning from **H**uman **F**eedback, **인간 피드백을 통한 강화 학습)** 방식은 LLM의 '태도'와 '행동' 문제를 해결합니다.

초기 LLM은 인터넷의 온갖 데이터를 학습했기 때문에, 종종 편향적이거나, 공격적이거나, 아예 엉뚱한 답변을 내놓기도 했습니다. 따라서 LLM이 '인간이 선호하는 방식'으로 응답하게끔 조정하는 '정렬(Alignment)' 과정이 필요합니다.

RLHF는 이러한 ‘**정렬**’ 처리를 위한 핵심 기술이며, 결과적으로 모델이 ‘**유용하고, 무해하며, 정직하게(HHH**, **H**elpful, **H**armless, **H**onest)’ 행동하도록 유도합니다.

### 강화 학습(Reinforcement Learning)이란?

먼저, 강화 학습은 에이전트(Agent)가 환경과 상호작용하면서 보상(Reward)을 최대화하도록 학습하는 기계 학습(Machine Learning)의 한 분야입니다. 즉, '어떤 행동이 장기적으로 가장 좋은 결과를 가져오는가?'를 경험을 통해 스스로 학습하는 방법입니다.

> **비유: 강아지 훈련 🐶**
>
> 1. **에이전트(Agent):** 강아지
> 2. **행동(Action):** "손!"이라는 명령에 강아지가 손을 줍니다.
> 3. **보상(Reward):** 올바른 행동을 하자 '간식'을 줍니다.
> 4. **학습:** 강아지(에이전트)는 간식(보상)을 최대화하는 방향으로 자신의 '행동(정책)'을 스스로 수정해 나갑니다.
{: .prompt-tip }

RLHF는 이 원리를 LLM 훈련에 적용한 것입니다.

### RLHF의 3단계 프로세스

1. **SFT(Supervised Fine-Tuning, 지도 미세조정)**

    * 우선 LLM이 '지시'를 따르도록 기초 교육을 시킵니다. (강아지에게 '손!'이라는 명령어를 가르치는 단계)

    * 사람이 직접 '좋은 질문과 좋은 답변' 세트 수천~수만 개를 작성합니다.
        * Q: "트랜스포머가 뭐야?" / A: "트랜스포머는 2017년 'Attention Is All You Need' 논문에서…"
    
    * LLM이 이 '모범 답안'을 따라 하도록 학습시킵니다. (이를 'Instruction Tuning'이라고도 합니다.)

2. **보상 모델(Reward Model, RM) 학습**

    * 다음은 강아지에게 '간식(보상)'을 주는 주체, 즉 '평가자'를 만들 차례입니다.

    * SFT 모델(1단계)에게 하나의 질문을 주고, 4~5개의 다른 답변(A, B, C, D)을 생성하게 합니다.

    * 생성된 답변들에 대해 **사람**이 '선호도 순위'를 매깁니다(Human Feedback).
        * 예: "답변 중 A가 제일 좋아(1위), C가 그다음(2위), B는 별로(3위), D는 최악(4위)."

    * 이 '인간의 선호도 순위' 데이터를 모아서 별도의 ‘**보상 모델(RM)**’을 학습시킵니다. 이 보상 모델의 역할은 LLM이 생성한 답변을 보고 '인간이 이 답변을 얼마나 좋아할까?'를 **'점수'로 예측**하는 것입니다.

    * 이제 우리는 인간 대신 자동으로 '간식'을 줄 평가자(RM)를 얻었습니다!

3. **강화 학습(RL)을 통한 LLM 튜닝**

    * 이제 본격적인 '강아지 훈련'이 시작됩니다.

    * **에이전트 = 정책 모델**(1단계에서 조정한 LLM), **평가자 = 보상 모델**(2단계에서 만든 RM)

    * 정책 모델이 질문에 대해 어떤 답변(Action)을 생성하면, 보상 모델이 그 답변에 대한 '선호도 점수(Reward)'를 줍니다. 이때 훈련 시스템은 'PPO(Proximal Policy Optimization)'와 같은 강화 학습 알고리즘을 사용하여 **보상 점수를 극대화하는 방향으로 LLM의 파라미터를 조정**합니다.

    * **결과:** LLM은 RM(인간의 선호도)이 가장 높은 점수를 줄 만한, 즉 일반적으로 인간이 보기에 가장 '유용하고, 무해하며, 정직한' 답변을 생성하는 방향으로 '정렬'됩니다.

### RLHF를 넘어: RLAIF(RL from AI Feedback)

RLHF의 2단계에서 '인간'이 답변 순위를 매기는 것은 매우 느리고 비쌉니다. 그래서 최근에는 이 '인간 평가자'의 역할을 다른 AI가 대체하는 **RLAIF(RL** from **AI** **F**eedback, **AI 피드백을 통한 강화 학습)** 방식이 주목받고 있습니다.

Anthropic의 'CAI(Constitutional AI)'가 대표적인 예입니다. 이 방식은 AI에게 '유용성, 무해성, 정직성'과 같은 '헌법(Constitution)'을 주고, AI가 이 원칙에 따라 답변을 비판(Critique)하고 스스로 수정(Revision)하도록 SFT 단계를 거칩니다. 그 후, AI가 생성한 '선호도 데이터'를 바탕으로 RL을 수행하여 모델을 정렬시킵니다.

---

## 4부 끝

이번 포스트에서는 3부에서 확인한 LLM의 한계를 극복하는 두 가지 핵심 보완 기술을 알게 되었습니다.

* **RAG:** '오픈북 시험'처럼 외부 지식을 검색하여, LLM의 환각을 억제하고 최신 정보를 제공합니다.

* **RLHF/RLAIF:** 강화 학습을 통해 인간의 선호도를 반영, LLM을 안전하고 유용하게 정렬(Alignment)합니다.

지금까지 우리는 1부(기초) → 2부(트랜스포머 아키텍처) → 3부(LLM 소개) → 4부(LLM 고도화) 시리즈를 통해 **'언어'를 다루는 AI 모델의 발전 과정**을 따라왔습니다.

하지만 AI가 생성하는 것은 텍스트만이 아니죠. 우리가 보는 'AI가 그린 그림'은 어떻게 만들어질까요?

다음 5부 - VAE, GAN, Diffusion에서는 생성형 AI의 또 다른 큰 축인 **이미지 생성 모델**(AutoEncoder, VAE, GAN, Diffusion)의 세계로 들어가 보겠습니다.

---

> _이 글의 초안은 ChatGPT, Gemini 등 AI와 주고받은 질의응답 내용을 토대로 작성되었습니다._
{: .prompt-info }