---
title: "AI는 어떻게 '세상'을 생성하는가? (5) - VAE, GAN, Diffusion"
author: 1st-world
date: 2025-10-26 16:45:00 +0900
last_modified_at: 2025-11-02 17:15:00 +0900
categories: [Artificial Intelligence, Machine Learning]
tags: [llm, vector, autoencoder, vae, gan, diffusion]
pin: false
math: true
---

우리는 이전까지 4부에 걸쳐 AI가 언어를 '이해'하고([1부](/posts/ai1-how-does-ai-understand-language/) / [2부](/posts/ai2-transformers-the-heart-of-modern-ai/)), '생성'하며([3부](/posts/ai3-the-emergence-of-llm/)), '정렬'되는([4부](/posts/ai4-rag-rlhf-make-llm-smarter/)) 과정을 탐험했습니다. 이 모든 과정의 중심에는 '트랜스포머'와 'LLM'이 있었죠.

지금까지의 시리즈가 주로 '언어(Text)'를 다루는 LLM의 발전 과정이었다면, 이번 5부는 **'시각(Image)'**을 중심으로 AI가 어떻게 무(無)에서 유(有)를 창조해내는지, 즉 '생성형 AI'의 또 다른 거대한 축을 다룹니다. "노을 지는 해변에서 코딩하는 고양이" 같은 상상 속 이미지를 순식간에 그려내는 Midjourney, DALL-E, Stable Diffusion은 어떻게 동작하는 걸까요?

이 모델들은 LLM과는 전혀 다른 방식으로 '생성'을 배웁니다. 핵심은 **'잠재 공간(Latent Space)'**이라는 개념을 어떻게 더 잘 학습하고 활용하느냐의 싸움입니다.

> **핵심 개념: 잠재 공간(Latent Space)**
>
> '잠재 공간'이란, 데이터의 **핵심 특징(Essence)**만을 압축해 놓은 다차원 공간을 의미합니다.
>
> 비유하자면, '자동차' 사진(1024×1024px = 약 300만 개의 숫자)을 "바퀴 4개, 문 4개, 세단 형태, 빨간색"이라는 **512개의 숫자(벡터)** 형태로 압축하는 것과 같습니다. 이 압축된 벡터가 바로 '잠재 공간'의 한 점입니다.
>
> 이미지 생성 모델의 목표는 '잠재 공간'을 학습한 뒤 **이 공간의 무작위 점을 골라 디코딩(Decoding)함으로써** 세상에 존재하지 않던 새로운 이미지를 만들어내는 것입니다.
{: .prompt-tip }

오늘은 이 '잠재 공간'을 다루는 방식에 따라 진화해 온 3가지 핵심 모델, **VAE, GAN, Diffusion**을 알아보겠습니다.

---

## 1. AE(AutoEncoder): '압축'은 잘하지만 '창조'는 못하는 모델

모든 것의 시작은 **'오토인코더(AE, AutoEncoder)'**입니다. AE는 '생성'이 아닌 '압축'을 위해 태어났습니다.

* **구조:** [2부](/posts/ai2-transformers-the-heart-of-modern-ai/)의 트랜스포머처럼 **'인코더(Encoder)'**와 **'디코더(Decoder)'**로 구성됩니다.
    * **인코더:** 원본 이미지($x$)를 입력받아 '잠재 벡터($z$)'로 **압축**합니다.
    * **디코더:** 잠재 벡터($z$)를 입력받아 원본 이미지($x'$)로 **복원**합니다.

* **학습 목표:** 입력($x$)과 출력($x'$)이 **똑같아지도록** 학습합니다. ($x \approx x'$)

* **치명적 한계:** AE의 잠재 공간은 학습 데이터 근처에서만 유효하므로 연속적이지 않고 샘플링 시 비어 있는 영역이 많습니다. 즉, AE가 학습한 잠재 공간은 '생성'에 적합하지 않습니다. 예를 들어 '고양이'를 압축한 벡터와 '개'를 압축한 벡터 사이의 중간 지점에서 벡터를 뽑으면, 의미 있는 '개냥이'가 나오는 것이 아니라 그냥 깨진 이미지가 나옵니다.

---

## 2. VAE(Variational AutoEncoder): '창조'가 가능한 잠재 공간

AE의 한계를 극복한 것이 **'변이형 오토인코더(VAE)'**입니다. VAE는 '생성'을 염두에 둔 첫 번째 성공작입니다.

* **핵심 아이디어:** VAE의 인코더는 AE와는 달리 '하나의 정확한 벡터($z$)'를 출력하지 않습니다. 대신, 잠재 벡터가 존재할 만한 '확률 분포'를 정의하는 파라미터, 즉, '평균($\mu$) 벡터와 분산($\sigma^2$) 벡터'를 출력합니다.
    * **비유:** AE가 "고양이 벡터는 정확히 (1.5, 2.1)이야!"라고 말한다면, VAE는 "고양이 벡터는 중심 (1.5, 2.1), 분산 (0.3)의 정규 분포 안에 있어!"라고 말합니다.
    * 평균($\mu$) 벡터와 분산($\sigma^2$) 벡터를 출력한다고 표현했지만, 정확히는 분산 자체보다는 그 로그($\log \sigma^2$)를 출력해 수치적 안정성을 높입니다.

* **효과:** 인코더가 출력한 **평균과 분산을 따르는 정규 분포로부터 실제 잠재 벡터 $z$를 '샘플링(Sampling)'하는 과정**(정확히는 '재매개변수화 기법(Reparameterization Trick)'을 사용)을 학습에 포함함으로써, 모델은 잠재 공간을 연속적으로 빽빽하게(Continuous & Dense) 채우도록 학습됩니다. 앞선 비유로 치면 결과적으로 '고양이' 영역과 '개' 영역이 자연스럽게 연결되는 것입니다.

### 어떻게 학습할까요? ― 이중 손실 함수

VAE는 '손실 함수(Loss Function)'에 AE와 다른 특별한 장치를 추가합니다.

#### Reconstruction Loss(복원 손실)

* AE와 동일한 목표입니다. 디코더가 원본 이미지($x$)를 얼마나 잘 복원($x'$)했는지 측정합니다. ($x \approx x'$)

#### KL-Divergence(쿨백-라이블러 발산)

* 이것이 **VAE의 핵심이자 'V(Variational)'의 이유**입니다.
    * 인코더가 **사후 분포(Posterior) $q(z\|x)$를 근사한다**는 점에서 'Variational'이라는 이름이 붙었습니다.

* KL-Divergence($D_{KL}$)는 '두 확률 분포가 얼마나 다른지'를 측정하는 지표이며, VAE에서는 **'정규화 벌칙(Penalty)'**으로 작용합니다.

* **비교 대상**
    1. **인코더의 출력 분포 $q(z\|x)$**
        * '이 고양이' 이미지를 보고 인코더가 예측한 개별 분포 (예: $N(1.5, 0.3)$)

    2. **우리의 목표 분포 $p(z)$**
        * 우리가 만들고 싶은 '이상적인 잠재 공간'의 분포 (보통 **평균 0, 분산 1의 표준 정규 분포, $N(0, 1)$**)

* **벌칙 작동 방식:** 예를 들어 인코더가 마치 AE처럼 "이 고양이는 평균 10.0, 분산 0.01이야!"라고(분산이 작음) 중심(0)에서 먼 값을 출력하면, $N(10.0, 0.01)$과 $N(0, 1)$의 KL-Divergence 값이 매우 커져 '벌점'을 세게 받습니다.

* **학습 효과:** 모델은 전체 VAE Loss를 줄여야 하므로, 이 '벌점'을 줄이기 위해 인코더가 **모든 이미지**('고양이', '개' 등)**의 분포를 최대한 $N(0, 1)$ 근처**로 모이도록 학습시킵니다.

* 이 KL-Divergence 항 덕분에 '고양이' 클러스터와 '개' 클러스터가 잠재 공간의 원점 근처에 연속적으로 빽빽하게(Continuous & Dense) 채워지게 됩니다.

### 생성 방법

* 학습이 끝난 VAE의 **디코더**만 가져옵니다.
* KL-Divergence 덕분에 잠재 공간이 $N(0, 1)$ 근처에 잘 정돈된 것을 알았으므로 $N(0, 1)$에서 무작위 벡터 $z$를 '샘플링'하여 디코더에 넣으면, 디코더는 이 무작위 벡터를 해석하여 학습 데이터에 없었던 새로운 이미지를 생성합니다.

* 단, **생성된 이미지가 다소 '흐릿하다(Blurry)'라는 한계**가 있습니다.

### 왜 흐릿하게 생성되나요?

'흐릿함'은 두 손실 함수의 '줄다리기(Trade-off)' 결과입니다.

1. **Reconstruction Loss(복원 손실)**
    * "이미지를 픽셀 단위로 원본과 똑같이 복원해라!" (세부 묘사 강조)

2. **KL-Divergence(정규화 손실)**
    * "잠재 공간을 $N(0, 1)$ 근처로 빽빽하고 부드럽게 정돈해라!" (생성을 위한 '일반화' 강조)

모델이 1번 목표(복원)에만 집중하면 AutoEncoder(AE)처럼 잠재 공간이 엉망이 되어 '생성'을 할 수 없습니다. 그래서 2번 목표(KL-Divergence)가 "잠재 벡터를 $N(0, 1)$ 근처로 모아라!"라고 강력한 '벌칙(제약)'을 걸고, 모델은 이 벌칙을 피하기 위해 원본 이미지의 모든 디테일을 잠재 벡터에 욱여넣는 것을 포기합니다.

즉, **생성이 가능한(정규화된) 잠재 공간을 만드는 대가로, 원본 이미지의 날카로운 세부 묘사(High-frequency details)를 일부 손실**하게 되고, 이 손실된 디테일이 디코더를 통해 복원될 때 모델은 가장 '안전한' 선택, 즉 '평균적인' 값(예: 검은색과 흰색 사이에는 회색)을 선택하며, 이것이 우리 눈에 흐릿한 이미지로 보이는 거죠.

결국 '흐릿함'은 VAE가 생성 모델로서 기능하기 위해 지불하는 일종의 '비용'입니다.

이 한계를 개선하기 위해 VAE에 GAN의 디코더를 결합한 VAE-GAN, $\beta$-VAE 등의 변형도 등장했습니다.

---

## 3. GAN(Generative Adversarial Networks): '경쟁'을 통한 극사실주의

VAE가 '흐릿함'이라는 타협점을 가졌다면, **'적대적 생성 신경망(GAN)'**은 타협 없는 극사실주의를 목표로 하며 세상을 놀라게 했습니다.

* **핵심 아이디어:** '생성자'와 '판별자'라는 두 모델을 **서로 '경쟁(Adversarial)'**시킵니다.

* **비유:** '위조지폐범(생성자)'과 '경찰(판별자)'의 대결입니다.
    * **생성자(Generator / 위조지폐범):** 무작위 노이즈(잠재 벡터 $z$)를 입력받아 '가짜 이미지 $G(z)$'를 만듭니다. (경찰을 속이는 것이 목표)
    * **판별자(Discriminator / 경찰):** '진짜 이미지 $x$' 또는 '가짜 이미지 $G(z)$'를 입력받아, 이것이 **진짜일 확률**(0.0~1.0)을 출력합니다. (위조지폐를 걸러내는 것이 목표)

### 어떻게 학습할까요? ― 제로섬 게임(Minimax Loss)

GAN의 학습 과정은 VAE의 '이중 손실 함수'와 달리, 두 플레이어가 하나의 목표를 두고 싸우는 **'미니맥스(Minimax)' 형태의 제로섬 게임**으로 이해할 수 있습니다.

하나의 **가치 함수(Value Function)**를 두고, 두 모델은 서로 반대되는 목표를 가집니다.

$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$$

위 식이 바로 두 모델이 경쟁하는 '게임판'이며, $G$는 $V$를 최소화, $D$는 $V$를 최대화하는 것이 목표입니다.

수식을 풀어서 각자의 목표로 나누어 보면 다음과 같습니다.

1. **판별자 $D$의 목표: Maximize $V$**
    * **첫 번째 항**($\mathbb{E}_{x \sim p\_{data}(x)}[\log D(x)]$): 진짜 이미지 $x$를 입력받았을 때, 그 판별값 $D(x)$가 1에 가까워지도록(즉, $\log(1)=0$이 되도록) 이 항을 최대화합니다.

    * **두 번째 항**($\mathbb{E}_{z \sim p\_z(z)}[\log(1 - D(G(z)))]$): 가짜 이미지 $G(z)$를 입력받았을 때, 그 판별값 $D(G(z))$가 0에 가까워지도록(즉, $\log(1-0)=0$이 되도록) 이 항을 최대화합니다.

    * 결국, $D$는 진짜 이미지 $x$는 1에 가깝게, 가짜 이미지 $G(z)$는 0에 가깝게 판별하여 $V(D, G)$를 **최대화**하려 합니다.

2. **생성자 $G$의 목표: Minimize $V$**
    * 생성자는 진짜 이미지 $x$를 만들지 않으므로 첫 번째 항($\log D(x)$)에는 영향을 줄 수 없습니다. 오직 두 번째 항($\mathbb{E}_{z \sim p\_z(z)}[\log(1 - D(G(z)))]$)에만 영향을 줍니다.

    * $G$는 $D$를 속여서 $D(G(z))$를 1에 가깝게 만들려 합니다. $D(G(z))$가 1이 되면 $\log(1-1) = \log(0) \rightarrow -\infty$가 되므로, $V(D, G)$ 전체를 **최소화**하게 됩니다.

### 학습 과정 ― 불안정한 '균형' 찾기

학습은 두 모델이 번갈아가며, 즉 '차례(Turn)'를 주고받으며 진행됩니다.

1. **판별자 차례:** 생성자 $G$를 잠시 '고정'시키고, 판별자 $D$가 진짜와 가짜를 더 잘 구별할 수 있도록 몇 Step 학습시킵니다. (경찰이 위조지폐 감별법을 배우는 것)

2. **생성자 차례:** 판별자 $D$를 잠시 '고정'시키고, 생성자 $G$가 (더 똑똑해진) 판별자를 속일 수 있는 더 정교한 가짜 이미지를 만들도록 학습시킵니다. (위조지폐범이 신기술을 개발하는 것)

이 경쟁이 이론적인 '내쉬 균형(Nash Equilibrium)'에 도달하면, 생성자는 너무나 실제 같은 가짜 이미지를 만들어서, 판별자가 최선을 다해 판별해도 정답 확률이 **50%** (즉, 찍는 것과 다름없는) 수준이 됩니다.

이는 판별자가 '무력'해진 것이 아니라, 생성자가 '완벽'해졌다는 것을 의미합니다. 수학적으로 이 균형점은 손실 함수의 **'안장점(Saddle Point)'**(한쪽에서는 최솟값, 다른 쪽에서는 최댓값이 되는 지점)을 찾는 과정과 같습니다.

### 강점과 한계점

* VAE와 달리 GAN은 '복원 손실(Reconstruction Loss)'이 없습니다. 생성자는 오직 '판별자를 속인다'는 단 하나의 목표(가짜를 진짜처럼)에만 집중합니다. 따라서 VAE의 '흐릿함'(픽셀 단위 평균화) 문제가 없는, **매우 선명(Sharp)하고 사실적인 이미지**를 생성할 수 있습니다.

* 단, 두 모델(생성자와 판별자)의 균형을 맞추며 훈련시키는 것이 **극도로 어렵고 불안정**합니다. 생성자가 데이터의 일부 패턴에만 집중하는 **'모드 붕괴(Mode Collapse)'** 등의 문제도 자주 발생합니다.

### 왜 어렵고 불안정한가요?

GAN의 강점인 '경쟁'은 동시에 **가장 치명적인 약점**인 '불안정성'의 원인이 됩니다.

#### 1. Vanishing Gradients

'경찰(판별자 $D$)'과 '위조지폐범(생성자 $G$)'의 균형은 매우 민감합니다.

* **문제:** 만약 '경찰'이 초반부터 너무 똑똑해져서 '위조지폐범'이 만든 가짜 이미지를 100% 확률로 "이건 가짜야!"라고 완벽하게 잡아낸다고 가정해 봅시다.

* **결과:** '위조지폐범' 입장에서는 "내가 뭘 만들든 어차피 0점이네"가 됩니다. 자신이 무엇을 잘못했는지, 어떻게 개선해야 할지에 대한 피드백(기울기 신호)을 받지 못해 **학습이 멈춰버립니다.**

> **비포화 손실(Non-Saturating Loss)**
>
> 이 Vanishing Gradients 문제를 완화하기 위해, 실제 GAN 구현에서는 생성자 $G$의 손실 함수를 $min \log(1-D(G(z)))$ 대신 $max \log(D(G(z)))$ 형태로 바꾸어 사용합니다.
>
> 이 **비포화(Non-saturating) 손실 함수**는 학습 초기에 생성자가 판별자에게 완전히 패배하고 있을 때 **더 강력한 기울기 신호**를 제공함으로써 학습이 멈추지 않도록 돕는 핵심 장치입니다.
{: .prompt-tip }

#### 2. Mode Collapse

이 '모드 붕괴' 현상은 GAN의 가장 고질적인 실패 사례입니다.

* **문제:** '위조지폐범'이 수많은 시도 끝에, '경찰'이 유독 잘 속아 넘어가는 **'특정 디자인의 100달러 지폐'** 하나를 완벽하게 위조하는 법을 발견했다고 가정해 봅시다.

* **원인(Exploitation):** '위조지폐범'의 목표는 '다양한 종류의 지폐'를 만드는 것이 아니라 **오직 '경찰'을 속이는 것**입니다. 괜히 10달러나 50달러 지폐를 만들다가 '경찰'에게 들킬 위험을 감수할 이유가 없습니다.

* **결과:** '위조지폐범'은 가장 안전한 '그 100달러 지폐'만 **반복적으로 생성**하기 시작합니다. 이것이 생성자가 다양성(Diversity)을 상실하고 '하나의 모드(Mode)'에 갇혀버리는 **'모드 붕괴(Mode Collapse)'** 현상입니다.

결국, GAN의 학습은 '경찰'이 너무 똑똑해도 안 되고 '위조지폐범'이 너무 한 우물만 파도 안 되는, 매우 불안정한 '균형점'을 찾아가는 과정이기에 어려운 것입니다.

이후에 이러한 문제들을 개선하기 위한 WGAN, LSGAN 등 많은 변형 모델들도 등장했습니다.

---

## 4. Diffusion Model: '노이즈'에서 '명작'을 빚어내다

마침내, 현대 이미지 생성 AI의 왕좌를 차지한 **'확산 모델(Diffusion Model)'**이 등장합니다. DALL-E 2, Midjourney, Stable Diffusion 등 수많은 모델들이 이 방식을 기반으로 하고 있습니다.

* **핵심 아이디어:** 이미지를 한 번에 생성하는 것이 아니라, '완전한 노이즈'에서 시작해 **단계별로 노이즈를 제거(Denoising)**하여 이미지를 '조각'해냅니다.
    * **비유:** 빗소리(노이즈)만 가득한 녹음 파일에서, 빗소리를 섬세하게 단계적으로 제거했더니 원래는 들리지 않았던 '사람의 목소리'가 드러나는 것과 같습니다.

Diffusion의 학습 과정은 크게 **2단계(순방향/역방향 확산)**로 나눌 수 있습니다.

### 순방향 확산(Forward Diffusion)

* 이 과정은 모델 파라미터를 학습하는 것이 아닌, 미리 정의된 노이즈 스케줄(Noise schedule)에 따르는 **고정된(Fixed) 비학습적(Non-Learning) 프로세스**입니다.

* **핵심 개념: Markov Process**
    * 컴퓨터는 '고양이' 이미지($x_0$)를 가져와 총 Step 수 $T$번(예: 1000번)에 걸쳐 이미지를 서서히 '파괴'합니다. 이때 **'가우시안 노이즈(Gaussian Noise)'**를 재료로 사용합니다.

    * $t=1$일 때, $x_0$(원본)에 가우시안 노이즈를 1%만큼 섞어 $x_1$을 만듭니다.
    * $t=2$일 때, $x_1$에 가우시안 노이즈를 1.1%만큼 섞어 $x_2$를 만듭니다.
    * …
    * $t=1000$일 때, $x_{999}$에 가우시안 노이즈를 30%만큼 섞어 $x_{1000}$을 만듭니다. 이렇게 만들어진 $x_{1000}$은 사실상 완전한 노이즈 덩어리가 됩니다.

* **문제점:** 나중에 모델을 학습시킬 때, ($T=1000$이라고 가정하면) $t$를 1부터 1000 사이에서 매번 무작위로 하나 선택합니다. 이때 예를 들어 $t=350$ 시점의 이미지가 요구된다면, 이 이미지를 얻기 위해 1단계부터 350단계를 **전부** 계산하는 것은 너무 비효율적입니다.

* **핵심 트릭: The 'Jump' Formula**
    * 다행히 우리는 수학적으로 다루기 편리한 '가우시안 노이즈'를 사용했기 때문에, 이 350단계를 모두 거칠 필요 없이 **단 한 번의 계산만으로 $x_0$에서 $x_{350}$으로 '점프'**할 수 있는 공식이 존재합니다.

    $$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$$

#### '점프 공식' 파헤치기

이 수식이 복잡해 보일 수 있지만, 사실 **'이미지와 노이즈의 비중을 조절하는 가중 평균'**일 뿐입니다.

$$x_t = ( \text{원본 이미지 비중} ) \times ( \text{원본 이미지} ) + ( \text{노이즈 비중} ) \times ( \text{노이즈} )$$

이제 위 공식을 이 가이드에 맞춰 하나씩 다시 보겠습니다.

1. **$x_0$ :** 원본 '고양이' 이미지입니다.
2. **$\epsilon$ (엡실론):** 100% 무작위인 **'가우시안 노이즈'**입니다. ($\epsilon \sim N(0, I)$는 '평균 0, 분산 1의 표준 정규분포(가우시안) 노이즈'라는 수학적 표기입니다.)
3. **$\bar{\alpha}_t$ (알파-바):** **'시그널(Signal) 비중'** 또는 **'원본 이미지 잔존량'**입니다.
    * 이 값은 $t$ (시간)에 따라 미리 정해져 있습니다. (e.g., $T=1000$ 스케줄)
    * $t=1$일 때, $\bar{\alpha}_t \approx 0.99$ (이미지 비중 99%)
    * $t=350$일 때, $\bar{\alpha}_t \approx 0.3$ (이미지 비중 30%)
    * $t=1000$일 때, $\bar{\alpha}_t \approx 0.0$ (이미지 비중 0%)
4. **$\sqrt{\bar{\alpha}_t}$ (루트 알파-바):** '원본 이미지'($x_0$)에 곱해줄 **'원본 비중 값'**입니다.
5. **$\sqrt{1 - \bar{\alpha}_t}$ :** '노이즈'($\epsilon$)에 곱해줄 **'노이즈 비중 값'**입니다.

**예시:** 만약 $t=350$일 때 $\bar{\alpha}_{350}=0.3$이라면, 위 공식은 다음과 같이 번역됩니다.

$$x_{350} = \sqrt{0.3} \times (x_0) + \sqrt{1 - 0.3} \times (\epsilon)$$

$$x_{350} = (0.55 \times \text{원본 이미지}) + (0.84 \times \text{노이즈})$$

즉, 원본 이미지의 비중을 55%로 줄이고 노이즈의 비중을 84%로 섞으면 350단계의 노이즈 낀 이미지를 한 번에 만들어낼 수 있다는 것입니다.

이 '점프 공식' 덕분에 모델 학습 과정이 훨씬 간단해질 수 있게 되었습니다.

---

### 역방향 확산(Reverse Diffusion)

* 이 과정이 **AI 모델(U-Net)이 '학습'하는 프로세스**입니다.
* 모델은 $t$ 시점의 노이즈 낀 이미지 $x_t$와 시점 $t$를 입력받아, $x_t$를 만드는 데 사용된 **원본 노이즈($\epsilon$)**를 예측하도록 학습됩니다.

#### U-Net은 무엇일까요?

Diffusion 모델의 AI는 대부분 **U-Net**이라는 특수한 아키텍처를 사용합니다.

* **구조:** U-Net의 기본 구조는 AE처럼 '인코더-디코더'를 가집니다(이미지의 문맥을 압축했다가 다시 펼침). 다만 [1부](/posts/ai1-how-does-ai-understand-language/)에서 소개했던 ResNet처럼 **'스킵 커넥션(Skip Connection)'**을 가지고 있는 것이 핵심입니다.

* **효과:** 인코더에서 '저수준(Low-level) 특징'(예: 고양이의 털 질감, 윤곽선 등)이 스킵 커넥션을 통해 **디코더로 그대로 전달**됩니다. 덕분에 디코더는 '고수준(High-level) 문맥'(예: "이건 고양이 형체다")과 '저수준 질감'을 모두 참고하여 각 픽셀의 노이즈에 대해 정교하게 예측할 수 있습니다. 

#### 왜 GAN보다 안정적일까요?

GAN이 두 모델의 균형을 맞추는 민감한 작업인 반면 Diffusion의 학습은 훨씬 단순하고 안정적입니다.

* **손실 함수:** **평균 제곱 오차(MSE, Mean Squared Error)** (또는 L1/L2 Loss)를 사용합니다.
    * $Loss = \mathbb{E}\_{x_0, t, \epsilon} \left[ \Vert \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) \Vert^2 \right]$

    * **$\epsilon_\theta(x_t, t)$ :** AI 모델(U-Net)이 $x_t$와 $t$를 보고 예측한 노이즈
    * **$\epsilon$ :** $x_t$를 만들 때 사용된 '실제 정답' 노이즈

* **의미:** AI가 예측한 노이즈 벡터와 실제 정답 노이즈 벡터가 **얼마나 다른지**에 대해서만 계산하여 그 '차이'를 줄이는 방향으로 학습합니다.

* **효과:** GAN의 고질적 문제인 학습 불안정성 및 '모드 붕괴'가 일어날 위험이 거의 없는 **안정적인(Stable) 지도 학습**입니다. 이것이 Diffusion이 GAN을 제치고 대세가 된 가장 큰 이유입니다.

### 생성 방법

1. 완전히 무작위인 **'순수 노이즈'**($x_T$)에서 시작합니다. (Step 1000)
2. AI 모델(U-Net)에게 $x_T$와 $t=T$를 입력하면, AI는 "이 노이즈를 제거하려면 이런 노이즈($\epsilon_\theta$)를 빼야 합니다"라고 예측합니다.
3. $x_T$에서 이 예측된 노이즈를 '적절히' 빼서 **살짝 깨끗해진 노이즈**($x_{T-1}$)를 계산합니다. (수학적으로는 $x_{T-1}$을 추정하는 별도 공식을 사용합니다.)
4. 이 과정을 $T$번 반복하면($x_T \rightarrow x_{T-1} \rightarrow \dots \rightarrow x_1 \rightarrow x_0$), 노이즈가 단계적으로 걷히면서 최종적으로 선명한 이미지($x_0$)가 나타나게 됩니다.

> **'Step 1000'의 의미**
>
> '1000'이라는 숫자는 아무 의미 없는 예시가 아니라 **실제로 널리 사용되는 기준**입니다.
>
> 2020년, Diffusion 모델의 기반이 된 “[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)” 논문에서 사용한 총 Step 수 $T$가 1000이었습니다. 이 $T=1000$ 스케줄은 이후 많은 모델의 표준적인 학습 설정이 되었습니다.
>
> 단, 학습 시 Step을 1000으로 사용했다고 해서 이미지 생성 시(추론)에도 1000번의 '노이즈 제거' 단계를 모두 거치면 **이미지 한 장에 수 분이 걸릴 정도로 매우 느립니다.** 이후 개발된 DDIM(Denoising Diffusion Implicit Models) 같은 '고속 샘플러'들은 $T=1000$ 스케줄을 완전히 따르지 않고, 1000개 중 50개, 20개 등 **일부 Step만 '건너뛰며' 샘플링**해도 준수한 품질의 이미지를 생성할 수 있음을 보여주었습니다. Stable Diffusion 등에서 설정하는 "Sampling Steps: 50" 같은 옵션이 바로 이것이며, **속도와 품질의 Trade-off** 관계입니다.
{: .prompt-tip }

---

### CFG(Classifier-Free Guidance)

"해변에서 코딩하는 고양이"라는 프롬프트를 어떻게 따르게 할까요? 단순히 '조건(Condition)'으로 임베딩 벡터를 넣어주는 것 외에도, Stable Diffusion 등은 **'Classifier-Free Guidance(CFG)'**라는 핵심 트릭을 사용합니다.

AI는 학습할 때 두 가지 경우(Conditional, Unconditional)를 모두 훈련합니다.

* **Conditional:** '프롬프트(텍스트)'와 '노이즈 낀 이미지'를 함께 입력받아 노이즈를 예측합니다.
* **Unconditional:** '빈 프롬프트(비어있음)'와 '노이즈 낀 이미지'를 입력받아 노이즈를 예측합니다. (즉, 프롬프트 없이 순수하게 노이즈만 제거)

이후 생성 단계에서 모델은 **두 가지 예측을 동시에** 수행합니다.

* **$\epsilon_{\theta}(x_t, t, \text{"고양이"})$**: "코딩하는 고양이" 프롬프트가 **있는** 상태의 노이즈 예측
* **$\epsilon_{\theta}(x_t, t, \text{""})$**: 프롬프트가 **없는** (무작위) 상태의 노이즈 예측

그리고 이 두 예측 사이의 '차이 벡터'($\epsilon_{\theta}(\text{"고양이"}) - \epsilon_{\theta}(\text{""})$)를 계산합니다. 이 벡터는 무작위(Random) 이미지에서 '고양이' 이미지로 향하는 **방향**을 의미합니다. 최종 예측은 다음과 같습니다.

$$\tilde{\epsilon}_\theta = \epsilon_{\theta}(\text{“”}) + w \cdot (\epsilon_{\theta}(\text{“고양이”}) - \epsilon_{\theta}(\text{“”}))$$

여기서 **$w$**가 원본 논문에서 제안한 **'가이던스 가중치(Guidance Weight)'**이며, 우리가 흔히 **'CFG Scale'**이라고 부르는 값(보통 7~15)입니다.

* **Scale 값($w$)이 높으면** 무작위 상태에서 '고양이' 방향으로 더 강하게 밀어붙여 **프롬프트에 충실한** 이미지를 만듭니다.
* **Scale 값($w$)이 낮으면** 프롬프트의 영향력을 줄이고 AI가 더 **창의적(무작위)인** 이미지를 만듭니다.

이 CFG 트릭 덕분에 Diffusion 모델은 사용자의 지시를 강력히 따를 수 있게 되었습니다.

---

## 5부 끝

이번 포스트에서는 언어 모델과는 다른 경로로 발전해 온 이미지 생성 모델의 3대장을 알아보았습니다.

1. **VAE:** '확률적 잠재 공간'을 도입해 '창조'의 문을 열었습니다.
2. **GAN:** '적대적 경쟁'을 통해 '극사실적 이미지'를 생성했습니다.
3. **Diffusion:** '단계적 노이즈 제거' 방식으로 '안정성'과 '고품질'을 모두 잡으며 주류가 되었습니다.

지금까지 우리는 언어를 다루는 AI의 출발점부터 거대 언어 모델(LLM), 그리고 이미지 생성 모델에 이르기까지, 현대 AI의 핵심 모델들을 대부분 살펴보았습니다.

그렇다면 이 모델들이 "얼마나 좋은지", "얼마나 정확한지"는 어떻게 알 수 있을까요?

다음 [6부 - AI 평가의 핵심 지표](/posts/ai6-how-to-evaluate-ai-models/)에서는 Accuracy, Precision, Recall, F1, ROC/AUC 등 **AI 모델을 '평가'하는 지표들**에 관해 알아보면서 길고 길었던 본 시리즈의 여정을 마치도록 하겠습니다.
