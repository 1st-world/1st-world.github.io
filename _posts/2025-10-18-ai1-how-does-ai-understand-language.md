---
title: "AI는 어떻게 언어를 이해하는가? (1) - 임베딩과 잔차 연결"
author: 1st-world
date: 2025-10-18 16:40:00 +0900
last_modified_at: 2025-10-31 08:40:00 +0900
categories: [Artificial Intelligence, Machine Learning]
tags: [nlp, transformer, vector, token, embedding, gradient, resnet]
pin: false
math: true
---

ChatGPT, Gemini, Claude 등 현재 AI 모델들은 단순히 정보를 검색하는 것을 넘어, 마치 사람처럼 대화하고 문맥을 이해하며, 새로운 글을 창작하기도 합니다. 어떻게 기계가 이토록 정교하게 '언어'를 다룰 수 있게 된 걸까요?

컴퓨터는 0과 1로 이루어진 기계입니다. '사랑', '세계', '코딩' 같은 추상적인 단어의 의미를 그 자체로 이해할 수 없습니다. 그래서 AI가 언어를 이해한다는 것은 **단어와 문장을 컴퓨터가 처리할 수 있는 '숫자'의 형태로 바꾸는** 정교한 과정에서 시작됩니다.

이번 AI 시리즈의 첫 번째 포스트에서는 현대 AI, 특히 트랜스포머(Transformer) 아키텍처를 이해하기 위해 필요한 두 가지 핵심 부품, **임베딩(Embedding)**과 **잔차 연결(Residual Connection)**에 대해 알아보고자 합니다.

---

## 1. 언어 이해를 위한 첫걸음: 벡터화(Vectorize)

AI에게 특정 문장(예: "AI는 언어를 이해한다")을 가르치려면, 가장 먼저 이 문장을 조각내야 합니다.

### 1-1. 토큰화(Tokenize)

첫 단계는 문장을 의미 있는 최소 단위인 **토큰(Token)**으로 나누는 것입니다. 이 과정을 **'토큰화(Tokenize)'**라고 부릅니다. 레고로 성을 만들기 전에 블록을 하나하나 분리하는 것과 같습니다.

* "AI는 언어를 이해한다" → ["AI", "는", "언어", "를", "이해한다"]

이렇게 분리된 토큰들은 이제 숫자로 바뀔 준비를 마쳤습니다.

> **현대 모델의 토큰화 방식**
>
> 본문에서는 쉬운 이해를 위해 간단하게 어절 단위로 설명하고 있지만, BERT나 GPT 같은 현대 모델들은 **'서브워드(Subword)'** 토큰화 방식(예: BPE, SentencePiece)을 사용합니다.
>
> 이는 '이해한다'를 ['이해', '##한다']처럼 의미 있는 조각으로 쪼개는 방식입니다. 이렇게 하면 '이해했다', '이해하자' 등 다양한 변형을 효율적으로 처리하고, 사전에 없는 새로운 단어(OOV, Out-of-Vocabulary) 문제에 대해서도 유연하게 대응할 수 있습니다.
{: .prompt-tip }

### 1-2. 임베딩(Embedding): 단어를 '의미 좌표'에 배치하다

토큰을 숫자로 바꾸는 가장 단순한 방법은 각 단어에 번호를 매기는 것입니다.

* `{"AI": 1, "는": 2, "언어": 3, "를": 4, "이해한다": 5}`

하지만 이 방식은 치명적인 문제가 있습니다. 'AI(1)'와 '언어(3)'의 관계나, '이해한다(5)'와 '언어(3)'의 관계를 숫자가 전혀 표현하지 못합니다. 단지 식별 번호일 뿐이죠.

여기서 **'워드 임베딩(Word Embedding)'**이라는 천재적인 아이디어가 등장합니다. 단어를 하나의 숫자가 아닌, 수백 개의 숫자로 이루어진 **'벡터(Vector)'**로 표현하는 것입니다.

이 벡터는 일종의 '다차원 의미 공간' 속 좌표입니다.

> **비유: '의미 지도' 위의 좌표**
>
> '서울'이라는 단어는 [위도 37.5, 경도 127.0]이라는 좌표로 표현할 수 있습니다. '도쿄'는 [위도 35.6, 경도 139.7], '런던'은 [위도 51.5, 경도 -0.1] 정도로 표현되겠죠. 이 좌표(벡터)를 보면 '서울'에서 '도쿄'가 '런던'보다 지리적으로 가깝다는 것을 계산할 수 있습니다.
>
> 워드 임베딩은 이 개념을 '의미'로 확장합니다. '왕'과 '여왕'은 의미적으로 가깝고, '왕'과 '사과'는 멀도록 벡터 값을 부여합니다.
{: .prompt-tip }

이 임베딩 벡터는 놀라운 특징을 가집니다. **벡터 간 연산이 의미의 연산과 비슷하게** 동작한다는 것입니다.

* $Vector({\text{'왕'}}) - Vector({\text{'남자'}}) + Vector({\text{'여자'}}) \approx Vector({\text{'여왕'}})$
* $Vector({\text{'서울'}}) - Vector({\text{'한국'}}) + Vector({\text{'일본'}}) \approx Vector({\text{'도쿄'}})$

이처럼 텍스트(토큰)를 의미가 담긴 벡터(임베딩)로 변환하는 전체 과정을 **'벡터화(Vectorize)'**라고 합니다.

이제 AI는 단어가 갖는 의미를 숫자로 다룰 수 있게 되었습니다!

> **정적 임베딩 vs 문맥적 임베딩**
>
> 여기서 설명한 방식은 "bank"라는 단어가 어떤 문장에 쓰이든 **항상 동일한 벡터값**을 갖는 **'정적 임베딩(Static Embedding)'**입니다. (예: Word2Vec, GloVe)
>
> 하지만 "bank"의 의미는 상황에 따라 '은행(go to the bank)'이 될 수도, '강둑(on the river bank)'이 될 수도 있겠죠. 즉, '문맥'이 중요합니다.
>
> 2부에서 다룰 트랜스포머는, 이 정적 임베딩을 입력받아 **'문맥을 반영한 임베딩(Contextual Embedding)'**으로 동적으로 진화시킵니다. 이것이 현대 AI를 이루는 핵심 중 하나입니다.
{: .prompt-tip }

---

## 2. 깊은 신경망의 난제: 기울기 소실(Gradient Vanishing)

이제 단어를 숫자(벡터)로 바꿨으니, 이 숫자들을 신경망(Neural Network)에 입력하여 학습시킬 차례입니다.

신경망은 수많은 '층(Layer)'으로 구성됩니다. 일반적으로, 층이 깊어질수록(Deep Neural Network) 해당 모델은 더 복잡하고 추상적인 패턴을 학습할 수 있습니다. 더 똑똑해진다는 뜻입니다.

하지만 과거에는 신경망을 깊게 쌓을 수 없었습니다. **기울기 소실(Gradient Vanishing) 문제** 때문입니다.

> **비유: 산 정상에서 외치기**
>
> * **학습:** 신경망 학습은 산 정상(결과)에서 위치를 보고, 산 아래(입력)에 있는 수만 명의 일꾼(매개변수)들에게 "저쪽으로 세 걸음 가!"라고 외치는 것과 유사합니다.
>
> * **기울기(Gradient):** 이 '외침(피드백)'이 바로 '기울기'입니다. 이 신호가 산 아래까지 잘 전달되어야 일꾼들이 올바른 방향으로 움직일 수 있습니다(학습).
>
> * **기울기 소실:** 하지만 산이 너무 높으면(신경망이 너무 깊으면), 정상의 외침은 아래로 내려갈수록 점점 작아집니다. 특히 과거에 많이 사용했던 **'시그모이드(Sigmoid)' 같은 활성화 함수**는 일종의 '성능이 나쁜 중계기'와 같아서, **신호를 전달할 때마다 그 크기를 (0.9나 0.5처럼) 1보다 작은 값으로 줄여버렸습니다.** (수학적으로 미분값이 1보다 작음)
>
> * **결과:** 이 작은 숫자들이 수백 개의 층을 거치며 계속 곱해지면($0.5 \times 0.5 \times \dots$), 산기슭(초기 층)에 도달했을 때는 신호가 0에 수렴(기울기 소실)하며 일꾼들이 지시를 받지 못해 더 이상 움직이지 않게 됩니다(학습 실패).
{: .prompt-tip }

이 문제 때문에 층을 일정 수준 이상으로 늘리면 학습이 제대로 되지 않았습니다. '더 깊게 = 더 똑똑하게'라는 공식을 실현할 수 없었죠.

---

## 3. 혁신의 길: 잔차 연결(Residual Connection)

기울기 소실 문제를 어떻게 해결했을까요? 2015년에 'ResNet(Residual Networks)'을 소개하는 논문 “[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)”에서 아주 단순하면서도 강력한 아이디어가 제시되는데, 이것이 바로 **'잔차 연결(Residual Connection)'**입니다.

이 방법은 **신호(입력값 $x$)가 신경망 층($F(x)$)을 통과한 후, 그 층을 통과하기 전의 '원본 신호 $x$'를 다시 더해주는** 방식입니다.

* **기존 신경망:** ${\text{출력}} = F(x)$
* **잔차 연결:** ${\text{출력}} = F(x) + x$

이 간단한 덧셈 하나가 기울기 소실 문제를 효과적으로 완화합니다.

### 어떻게 이것이 가능할까요?

핵심은 학습 과정인 **역전파(Backpropagation)**에 있습니다. 학습 신호(기울기)는 출력층에서 입력층 방향으로 거꾸로 전달되죠.

* **신호의 두 갈래 길:** $F(x) + x$ 구조에서 기울기가 거꾸로 전달될 때 두 갈래 길을 만납니다.
    * 하나는 복잡한 신경망 층 $F(x)$를 통과하는 길입니다. 이 길은 신호가 약해질(기울기 소실) 위험이 있습니다.
    * 다른 하나는 $x$를 통해 아무런 변환 없이 입력으로 직행하는 '정체성(Identity) 경로'입니다.

* **기울기 보존:** $x$를 통해 전달되는 기울기가 그대로 입력층까지 도달할 수 있는 원리는 항등함수(Identity Function)의 미분($= 1$)이기 때문에 계산 과정을 거듭해도 감쇠되지 않는 것입니다.

* **결과:** 설령 신경망 층 $F(x)$를 통과하는 기울기가 0에 수렴하더라도, $x$를 통해 직행한 기울기 신호는 언제나 살아남습니다. 덕분에 아무리 층이 깊어져도 학습 신호가 초기 층까지 도달할 수 있게 됩니다.

> **왜 '잔차(Residual)'라고 부를까요?**
>
> 이 구조는 신경망 층($F(x)$)이 학습하는 '목표'를 바꿔줍니다.
>
> 기존에는 층이 입력 $x$를 받아서 최종 목표 $H(x)$를 전부 학습해야 했지만, 이 구조를 사용하면 층은 $F(x) = H(x) - x$ 만 학습하면 됩니다. 즉, **입력 $x$가 이미 갖고 있는 정보 위에 '남은 차이', 다시 말해 '잔여량'만 학습**하면 됩니다. 여기서 '잔차(Residual)'라는 이름이 붙었습니다.
>
> 입력값 $x$가 이미 정답과 꽤 비슷하다면 신경망 층 $F(x)$는 그 차이인 0에 가까운 작은 값만 학습하면 되므로 학습이 훨씬 쉬워집니다.
{: .prompt-tip }

이 간단한 $x$를 더해주는 아이디어 덕분에 신경망은 이제 수백 층 이상 깊어져도 학습이 가능해졌습니다.

---

## 1부 끝

지금까지 우리는 현대 AI를 떠받치는 두 개의 거대한 기둥을 배웠습니다.

* **임베딩(Embedding):** AI가 단어의 의미를 이해할 수 있도록 좌표(벡터)로 만드는 기술
* **잔차 연결(Residual Connection):** 기울기 소실을 해결하고 신경망을 깊이 학습시킬 수 있게 한 방법

다음 [2부 - 현대 AI의 심장, 트랜스포머](/posts/ai2-transformers-the-heart-of-modern-ai/)에서는 이 두 가지 핵심 부품을 토대로, 현존하는 거의 모든 LLM의 심장부라 할 수 있는 **'트랜스포머(Transformer)'** 구조에 관하여 본격적으로 살펴보겠습니다.
