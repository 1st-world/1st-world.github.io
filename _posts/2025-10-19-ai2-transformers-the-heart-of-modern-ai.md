---
title: "AI는 어떻게 언어를 이해하는가? (2) - 현대 AI의 심장, 트랜스포머"
author: 1st-world
date: 2025-10-19 23:25:00 +0900
last_modified_at: 2025-10-21 23:45:00 +0900
categories: [Artificial Intelligence, Machine Learning]
tags: [nlp, transformer, encoder, decoder, attention, softmax, resnet, ffn]
pin: false
---

지난 포스트 [1부 - 임베딩과 잔차 연결](/posts/ai1-how-does-ai-understand-language/)에서는 각 단어를 '의미가 담긴 숫자'로 바꾸는 **정적 임베딩(Static Embedding)** 과정과, 깊은 신경망의 학습을 가능하게 만든 **잔차 연결(Residual Connection)** 등 핵심 부품 몇몇을 다루었습니다.

이제 이 부품들을 토대로, 현대 AI의 판도를 바꾼 모델 '**트랜스포머(Transformer)**'를 만들어 볼 차례입니다.

2017년, 당시 Google 소속 과학자들이 "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)"라는 제목의 기념비적인 논문을 발표합니다. 이 논문에서 제안된 트랜스포머 아키텍처는 기존 RNN, LSTM의 한계를 극복하며 시퀀스 처리의 주류 구조로 자리 잡았습니다.

---

## 1. 트랜스포머의 근간: Encoder-Decoder

트랜스포머는 기본적으로 '번역기'와 같은 구조를 가집니다. "입력 문장(한국어)을 받아서, 출력 문장(영어)을 내놓는다"라는 아이디어죠. (실제로 트랜스포머는 원래 번역을 목적으로 연구되었습니다.)

이 구조는 크게 두 부분으로 나뉩니다.

1. **인코더(Encoder):** 입력 문장을 '이해'하고 '압축'하는 부분. (한국어 독해 전문가)

2. **디코더(Decoder):** 압축된 의미를 바탕으로 새로운 문장을 '생성'하는 부분. (영어 작문 전문가)

"나는 학생이다"라는 문장이 들어오면, 인코더는 이 문장을 읽고 그 의미를 풍부하게 담은 '숫자 덩어리(벡터)'로 압축합니다. 디코더는 이 '의미 덩어리'를 참조하여 "I am a student"라는 문장을 한 단어씩 생성합니다.

이제 각각의 부품을 더 자세히 살펴보겠습니다.

---

## 2. 인코더(Encoder): 문맥을 이해하는 뇌

인코더의 역할은 입력된 문장 속 단어들의 '진짜 의미', 즉 '**문맥적 임베딩(Contextual Embedding)**'을 만들어내는 것입니다.

### 2-1. 준비 단계: 임베딩 및 위치 인코딩(Positional Encoding)

우리는 1부에서 '정적 임베딩(Static Embedding)'에 관해 살펴본 적이 있었죠. 이때 "bank"라는 단어는 '은행'으로 쓰이든 '강둑'으로 쓰이든 동일한 벡터값을 가진다고 했고요.

"나는 학생이다"라는 문장이 입력되면, 인코더는 먼저 이 단어들을 정적 임베딩으로 변환합니다.

* `["나", "는", "학생", "이다"]` → `[Vector(나), Vector(는), Vector(학생), Vector(이다)]`

여기서 심각한 문제가 발생합니다. "학생은 나이다"와 "나는 학생이다"는 완전히 다른 문장이지만, 이 모델은 '순서'라는 개념이 없습니다. 단순히 단어 벡터들의 '집합'으로만 봅니다.

이 문제를 해결하기 위해 '**위치 인코딩(Positional Encoding)**'이 등장합니다.

* **작동 방식:** 각 단어의 위치를 나타내는 고유한 벡터를 만듭니다.

* **결과:** `[[Vector('나'), Vector(1번째)], [Vector('학생'), Vector(3번째)], …]`

* **비유:** '나'라는 단어에 1번가, '학생'이라는 단어에 3번가라는 **주소 태그**를 붙여주는 것입니다. 이제 모델은 "1번가에 있는 '나'"와 "3번가에 있는 '학생'"을 구분할 수 있게 됩니다.

> **위치 인코딩의 구현**
>
> 1. **사인($sin$), 코사인($cos$) 함수:** 논문에서 저자들은 고정된 $sin$, $cos$ 함수를 사용했습니다. 이 방식은 모델이 학습 중에 보지 못했던 더 긴 문장(예: 513번째 단어)이 들어와도 위치값을 계산해낼 수 있다는 장점이 있습니다.
>
> 2. **학습 가능한 임베딩(Learnable Embedding):** BERT나 GPT 같은 최신 모델 일부는 위치값 자체도 '학습 가능한 파라미터'로 둡니다. (예: 1번 위치 벡터, 2번 위치 벡터…를 데이터로부터 직접 학습)
{: .prompt-tip }

### 2-2. 핵심 단계: 셀프 어텐션 (Self-Attention)

주소 태그까지 붙인 단어 벡터들이 인코더의 핵심인 **'셀프 어텐션(Self-Attention)'** 층으로 들어갑니다.

이름 그대로, 문장 안의 단어들이 '**스스로(Self)**' 서로에게 '**주목(Attention)**'하며 문맥을 파악하는 과정입니다.

"The animal didn't cross the street because **it** was too tired."  
(그 동물은 길을 건너지 않았다, 왜냐하면 **그것**이 너무 피곤했기 때문이다.)

여기서 "**it**"은 "animal"일까요, "street"일까요? 사람은 "피곤한" 주체가 "동물"이라는 것을 바로 알죠.  
셀프 어텐션은 "it"이라는 단어가 문장 내 다른 단어들("animal", "street", "tired", …)을 쭉 훑어본 후 "아, 'it'은 'animal'과 90%만큼 관련 있고 'street'와는 5%만큼 관련 있구나"처럼 관계의 '가중치'를 계산합니다.

#### 어떻게 계산할까요? ($Q$, $K$, $V$)

이해를 돕기 위해 **사전 찾기**에 비유해 보겠습니다. 어텐션은 각 단어 벡터(예: 'it', 'animal', 'street', …)를 입력으로 하여, 세 개의 서로 다른 가중치 행렬($W_Q$, $W_K$, $W_V$)을 곱해 각각 $Q$, $K$, $V$ 벡터를 생성합니다.

1. **Query ($Q$): '검색어' 또는 '질문'**
    * 문맥을 파악하려는 '주체'입니다.
    * **예:** "it"이 자신의 문맥적 의미를 알기 위해 "나는 이 문장에서 '피곤할(tired) 수 있는' 주체를 찾고 있어!"라는 의미의 '질문(Query) 벡터'를 생성합니다.

2. **Key ($K$): 사전의 '색인 단어' 또는 '꼬리표'**
    * 다른 단어들이 "나와 관련 있는지 찾아보세요"라고 내미는 '키(Key) 벡터'입니다.
    * **예:** "animal"의 Key 벡터 → "나는 'animal'이고, 생명체이며, 피곤할 수 있음"이라는 속성을 나타냅니다.
    * **예:** "street"의 Key 벡터 → "나는 'street'이고, 사물이며, 피곤할 수 없음"이라는 속성을 나타냅니다.

3. **Value ($V$): 색인 단어에 해당하는 '실제 내용' 또는 '의미'**
    * Key가 성공적으로 매칭되었을 때, Query에게 전달해 줄 '정보(Value) 벡터'입니다.
    * **예:** "animal"의 Value 벡터 → 이 문맥에서 'animal'이 가진 풍부한 의미(예: '그 동물').
    * **예:** "street"의 Value 벡터 → 이 문맥에서 'street'이 가진 의미('길').

#### 계산 과정 (Scaled Dot-Product Attention)

1. "it"의 $Q$("피곤할 수 있는 주체인가?")가 "animal"의 $K$("나는 피곤할 수 있음") 및 "street"의 $K$("나는 피곤할 수 없음")와 얼마나 유사한지 '관련 점수(Score)'를 계산합니다. (내적/Dot-Product)

2. 당연히 $Q_{it}$와 $K_{animal}$의 점수가 $Q_{it}$와 $K_{street}$의 점수보다 훨씬 높게 나옵니다.

3. **스케일링(Scaling):** 이 점수를 $\frac{1}{\sqrt{d_k}}$ ($d_k$는 Key 벡터의 차원)로 계산합니다.
    * 스케일링을 하지 않으면 $d_k$가 커질수록 점수(내적 값)가 너무 커지면서 소프트맥스 함수의 출력이 0 또는 1에 극단적으로 치우치게 됩니다. 이렇게 되면 기울기 값이 작아져 학습에 문제를 일으킵니다. 스케일링은 이러한 문제를 방지하기 위한 안정화 장치입니다.

4. 이 점수들을 [**소프트맥스(Softmax) 함수**](/posts/softmax-cross-entropy-loss/)에 통과시켜 총합이 1이 되는 '어텐션 가중치(비율)'로 만듭니다. (예: animal = 0.9, street = 0.05, …)

5. 이 가중치(비율)를 각 단어의 $V$에 곱해서 모두 더합니다.

6. **결과:** "it"의 새로운 벡터는 $(0.9 * V_{animal}) + (0.05 * V_{street}) + …$ 처럼, 'animal'의 실제 의미(Value)가 90%나 섞인 '**문맥적 임베딩**'으로 재탄생합니다.

이 과정은 문장의 모든 단어에 대해 병렬적으로 수행됩니다. "tired"라는 단어 역시 "it"과 "animal"에 높은 가중치를 주겠죠. 이처럼 각 단어의 Query가 전체 단어의 Key, Value를 참조해 자신의 문맥적 임베딩을 계산합니다.

> **멀티 헤드 어텐션(Multi-Head Attention)**
>
> 실제로는 위 $Q$, $K$, $V$ 계산을 여러 세트(Head)로 동시에 수행합니다. 쉽게 말해 '여러 관점'에서 문맥을 파악한다는 것인데, 조금 더 정확히는 **각 헤드(Head)가 $Q$, $K$, $V$를 서로 다른 '부분 공간(Subspace)'으로 투영(Projection)하여** 학습합니다.
>
> 덕분에 어떤 헤드는 '주어-동사' 같은 문법적 관계에 집중하고, 다른 헤드는 '비슷한 의미' 같은 의미론적 관계에 집중하는 등, 더 풍부하게 문맥을 학습할 수 있습니다.
{: .prompt-tip }

### 2-3. 마무리: 잔차 연결, 정규화, 피드 포워드

이렇게 셀프 어텐션을 통해 '문맥'을 잔뜩 머금은 벡터가 나옵니다.

1. **잔차 연결 & 층 정규화(Add & Norm):** 1부에서 살펴본 잔차 연결이 여기서 쓰입니다.

    * **잔차 연결:** 어텐션을 통과하기 전의 원본 $x$와, 어텐션을 통과한 $x$를 더해줍니다. ($x + Attention(x)$). 기울기 흐름을 돕는 '고속도로' 역할입니다.

    * **층 정규화(Layer Normalization):** 잔차 연결된 결과($x + Attention(x)$)를 **'층 정규화(LayerNorm)'** 층에 통과시킵니다. (`LayerNorm(x + Attention(x))`)  
    LayerNorm은 각 층을 통과하는 데이터의 분포(평균, 분산)를 안정화시켜, 모델이 더 빠르고 안정적으로 학습될 수 있도록 돕는 '페이스 조절기' 역할을 합니다.

2. **피드 포워드(Feed Forward, FFN):** 정규화까지 마친 벡터가 '피드 포워드 신경망(FFN)'을 통과합니다.

    * 셀프 어텐션이 '회의(Networking)'를 통해 주변 정보를 수집한 것이라고 비유하자면, 피드 포워드는 각자 자기 자리로 돌아가 수집한 정보를 '소화(Digest)'하고 정리하는 시간입니다.

    * 이 FFN 블록 역시 **[잔차 연결 → 층 정규화]** 과정을 똑같이 거칩니다. (`LayerNorm(x' + FFN(x'))`)

이 **[셀프 어텐션 → Add & Norm → 피드 포워드 → Add & Norm]** 세트가 바로 **인코더의 1개 층**입니다. 트랜스포머는 이 인코더 층을 6개, 12개, 혹은 96개씩(GPT-3) 깊게 쌓아 올립니다.

---

## 3. 디코더(Decoder): 문맥을 생성하는 입

인코더가 입력 문장의 '의미 압축'을 끝냈습니다. 이제 디코더는 이 압축된 의미를 바탕으로 "I am a student"를 한 단어, 한 단어 생성해야 합니다.

디코더는 인코더와 거의 비슷하지만, '생성'을 위한 2가지 장치가 추가됩니다.

### 3-1. 마스크드 셀프 어텐션(Masked Self-Attention)

디코더도 인코더처럼 '셀프 어텐션'을 수행합니다. "I am a"까지 생성했다면, "am"이 "I"를 참고하고 "a"가 "I", "am"을 참고해야 합니다.

**하지만 똑같이 수행한다면 치명적인 문제(Cheating)가 발생합니다.** "I am a student"라는 정답을 학습할 때, "am"을 예측해야 하는 시점에 "a"나 "student"라는 **미래의 정답**을 봐버리게 되는 거죠.

그래서 해결책으로 **마스킹(Masking)** 기법을 사용합니다. 비유하자면, 문제를 풀 때 아직 풀지 않은 뒷장의 답안지를 가리는 것과 같습니다. "am"을 예측할 때는 "a"와 "student" 위치에 '마스크'를 씌워 정보가 흐르지 못하게 강제로 차단합니다. 이를 '미래를 보지 않는' **마스크드 셀프 어텐션**이라고 부릅니다.

> **마스킹의 구현**
>
> 기술적으로는 '어텐션 점수'를 계산한 직후, **소프트맥스를 적용하기 전에** 미래 시점에 해당하는 행렬 값에 아주 큰 음수(예: -1e9 또는 -∞)를 더합니다. 이 값을 소프트맥스에 통과시키면 해당 위치의 확률이 0이 되어 '미래 단어'의 Value 값이 무시됩니다.
{: .prompt-tip }

### 3-2. 인코더-디코더 어텐션(Cross-Attention)

디코더가 "I am"까지 생성했습니다. 이제 "나는 학생이다"라는 원본 문장의 의미를 참고할 차례입니다.

* 디코더의 '마스크드 셀프 어텐션'을 통과한 벡터가 Query($Q$)가 됩니다. ("'I am' 다음에 올 단어를 찾고 있어!")

* 인코더가 최종적으로 압축한 '의미 덩어리'가 Key($K$)와 Value($V$)가 됩니다. (원본 문장의 모든 정보)

디코더의 $Q$가 인코더의 $K$, $V$를 훑어보며 "지금 'I am' 다음 단어를 생성해야 하는데, 원본 문장 '나는 학생이다'에서 '학생' 부분이 가장 관련성이 높군!"이라고 판단합니다.

이것이 바로 인코더와 디코더가 만나는 '**인코더-디코더 어텐션**' (혹은 '**크로스 어텐션**')입니다.

### 3-3. 피드 포워드 및 최종 선택(Linear & Softmax)

1. **피드 포워드(FFN):** 인코더에서와 마찬가지로, 수집한 정보(자신이 생성한 단어 + 원본 문장의 의미)를 '소화'합니다. 이때 물론 Add & Norm 과정도 포함합니다.

2. **Linear & Softmax**
    * **Linear:** 디코더의 최종 출력 벡터가 나오면, 이 벡터를 Linear 층에 통과시켜, 우리가 가진 수만 개의 어휘(Vocabulary) 사전 크기의 벡터로 확장시킵니다. 각 단어별 '점수'가 매겨집니다.
    * **Softmax:** 이 점수들을 '확률'로 변환합니다. (총합 1)
    * **결과 예시:** `{"apple": 0.1%, "student": 95%, "I": 0.2%, …}`

3. 디코더는 확률이 가장 높은 "student"를 다음 단어로 선택하게 됩니다.

---

## 4. 생성 과정 (반복 및 종료)

이러한 과정은 디코더가 `<end>`라는 '문장 끝' 특수 토큰을 생성할 때까지 반복됩니다.

1. **입력:** 인코더가 "나는 학생이다"를 통째로 읽고 의미 압축

2. **T=1:** 디코더에 `<start>` 토큰 입력 (디코더 $Q$) $\times$ (인코더 $K$, $V$) → Softmax → "I" 선택

3. **T=2:** 디코더에 `<start>`, "I" 입력 (디코더 $Q$) $\times$ (인코더 $K$, $V$) → Softmax → "am" 선택

4. **T=3:** 디코더에 `<start>`, "I", "am" 입력 → "a" 선택

5. **T=4:** 디코더에 `<start>`, "I", "am", "a" 입력 → "student" 선택

6. **T=5:** 디코더에 `<start>`, "I", "am", "a", "student" 입력 → **`<end>` 선택**

7. **종료**

이처럼 이전 단계의 출력값을 다음 단계의 입력값으로 다시 사용하여 순차적으로 결과를 생성하는 방식을 **'자기 회귀적(Auto-regressive)' 생성**이라고 부릅니다.

---

## 2부 끝

지금까지 우리는 현대 AI의 핵심이라 할 수 있는 **트랜스포머**를 분해해 보았습니다.

* **인코더:** '셀프 어텐션'을 통해 입력 문장의 문맥을 이해합니다.

* **디코더:** '마스크드 셀프 어텐션'과 '인코더-디코더 어텐션'을 통해 문맥에 맞는 단어를 순차적으로 생성합니다.

* **핵심 안정화 장치:** 스케일링($\frac{1}{\sqrt{d_k}}$), 잔차 연결(Add), 층 정규화(Norm)가 깊은 층에서도 학습이 잘 되게끔 합니다.

이 트랜스포머 아키텍처는 원래의 목적이던 번역을 넘어 수많은 AI 분야의 기반이 되었습니다.

다음 포스트인 3부에서는 이 강력한 트랜스포머 구조를 약간 변형하고(디코더만 사용), 어마어마한 데이터로 학습시켰을 때 어떤 일이 벌어지는지, 즉 '**거대 언어 모델(LLM)**'의 탄생과 그 특징에 대해 알아보겠습니다.

---

> _이 글의 초안은 ChatGPT, Gemini 등 AI와 주고받은 질의응답 내용을 토대로 작성되었습니다._
{: .prompt-info }