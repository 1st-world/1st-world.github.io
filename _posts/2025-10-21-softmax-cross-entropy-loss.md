---
title: "소프트맥스와 교차 엔트로피 손실"
author: 1st-world
date: 2025-10-21 22:20:00 +0900
last_modified_at: 2025-10-22 13:55:00 +0900
categories: [Artificial Intelligence, Machine Learning]
tags: [softmax, cross-entropy]
pin: false
math: true
---

소프트맥스(Softmax) 함수와 교차 엔트로피 손실(Cross-Entropy Loss)은 머신러닝, 특히 **다중 클래스 분류(Multi-class classification)** 문제에서 사실상 표준처럼 함께 사용되는 한 쌍입니다.

---

## 1. 소프트맥스 함수(Softmax Function)

소프트맥스 함수는 모델이 계산한 **원시 점수(Raw score)** 또는 **로짓(Logit)** 값을 입력받아, 각 클래스에 대한 **확률 분포**로 변환하는 활성화 함수(Activation function)입니다.

예를 들어, 이미지를 보고 [고양이, 개, 새] 중 하나로 분류하는 모델이 있다고 가정해 봅시다. 이 모델이 내부적으로 계산한 원시 점수(로짓)는 다음과 같을 수 있습니다.

* 고양이: `3.2`
* 개: `1.3`
* 새: `0.5`

이 점수들은 직관적이지 않습니다. 여기서 소프트맥스 함수를 사용하면 이 점수들을 "그래서 각 항목일 확률이 몇 %인가?"로 바꿔줄 수 있게 됩니다.

### 주요 특징

1.  **확률 변환:** 모든 출력값은 0과 1 사이의 값을 가집니다.

2.  **총합은 1:** 모든 출력값의 합은 항상 1이 됩니다.

3.  **'Max' 강조:** '소프트맥스'에서 'Max'라는 이름처럼, 입력값 중 가장 큰 값을 확률적으로 더 크게 증폭시킵니다. (참고로 'Soft'는 가장 큰 값 하나만 1로 만드는 'Hard-Max'와 대비되는 의미입니다.)

### 수식

특정 클래스 $i$에 대한 소프트맥스 출력값 $S(y_i)$는 다음과 같이 계산됩니다.

$$S(y_i) = \frac{e^{y_i}}{\sum_{j=1}^{C} e^{y_j}}$$

* $y_i$: 클래스 $i$의 원시 점수(로짓).
* $e^{y_i}$: 지수 함수(Exponential function). 모든 값을 양수로 만들고, 큰 값과 작은 값의 차이를 증폭시킵니다.
* $\sum_{j=1}^{C} e^{y_j}$: 모든 클래스($C$개)의 지수 함수 값을 더한 총합. (정규화 항)

**예시 적용**

위의 [3.2, 1.3, 0.5] 로짓에 소프트맥스를 적용하면( $e^{3.2} \approx 24.53$, $e^{1.3} \approx 3.67$, $e^{0.5} \approx 1.65$ / 총합 $\approx 29.85$ ), 다음과 같은 확률을 얻을 수 있습니다.

* 고양이 확률: $24.53 / 29.85 \approx 0.822$ **(82.2%)**
* 개 확률: $3.67 / 29.85 \approx 0.123$ **(12.3%)**
* 새 확률: $1.65 / 29.85 \approx 0.055$ **(5.5%)**

> **실무적 참고: 수치 안정성(Numerical Stability)**
>
> 위 수식대로 $e^{3.2}$가 아닌 $e^{1000}$ 같은 큰 값을 계산하면 컴퓨터의 표현 범위를 넘어가는 **오버플로(overflow)** 문제가 발생합니다. 따라서 실제로 구현할 때는 이를 방지하기 위해 각 로짓에서 **로짓의 최댓값을 뺀 뒤** 지수 함수를 계산합니다. 일종의 트릭이라 할 수 있는 이 방법은 수학적으로 결과가 동일하면서도 오버플로를 막아줄 수 있습니다.
>
> $$S(y_i) = \frac{e^{y_i - \max(y)}}{\sum_{j=1}^{C} e^{y_j - \max(y)}}$$
{: .prompt-tip }

---

## 2. 교차 엔트로피 손실(Cross-Entropy Loss)

교차 엔트로피는 두 개의 확률 분포가 얼마나 다른지를 측정하는 방법입니다.

정보 이론 관점에서, 이는 ‘**실제 정답 분포 $t$를 모델이 예측한 분포 $p$로 근사(설명)할 때 필요한 평균 정보량(불확실성)**’을 의미합니다. 손실(Loss) 값이 낮을수록 모델이 실제 정답 분포를 잘 설명한다는 뜻입니다.

### 실제 정답 분포

다중 클래스 분류에서 실제 정답은 보통 **원-핫 인코딩(One-Hot Encoding)** 벡터로 표현됩니다. 만약 정답이 '고양이'라면, 정답 분포 $t$는 다음과 같습니다.

* $t$ = [고양이=1, 개=0, 새=0]

### 수식

하나의 데이터에 대한 교차 엔트로피 손실(CE)은 다음과 같이 계산됩니다.

$$CE = -\sum_{i=1}^{C} t_i \log(p_i)$$

* $C$: 전체 클래스의 수 (위 예시에서는 3)
* $t_i$: 클래스 $i$의 **실제 정답 분포**(Target distribution, 원-핫의 경우 0 또는 1)
* $p_i$: 클래스 $i$에 대해 모델이 예측한 확률 (소프트맥스 출력값)

### 직관적인 이해

위 수식은 복잡해 보이지만, 실제 정답 $t_i$는 하나만 1이고 나머지는 모두 0이기 때문에 매우 간단해집니다.

정답이 '고양이'($t_1=1$)일 때,

$$CE = - ( 1 \cdot \log(p_{\text{고양이}}) + 0 \cdot \log(p_{\text{개}}) + 0 \cdot \log(p_{\text{새}}) )$$

$$CE = - \log(p_{\text{고양이}})$$

즉, **교차 엔트로피 손실은 모델이 '정답'이라고 예측한 확률값에 $-\log$를 취한 것과 같습니다.**

* **예측을 잘한 경우** ($p_{\text{고양이}}=0.99$)
    * $Loss = -\log(0.99) \approx 0.01$ (손실이 매우 낮음)

* **예측을 못한 경우** ($p_{\text{고양이}}=0.01$)
    * $Loss = -\log(0.01) \approx 4.6$ (손실이 매우 높음)

이처럼 교차 엔트로피 손실은 모델이 정답을 틀리거나, 정답에 대한 확신(확률)이 낮을수록 엄청난 페널티(손실)를 부과하여 모델이 더 확실하게 정답을 맞히게끔 학습시킵니다.

참고로 이 수식은 $t$가 [0.7, 0.2, 0.1]처럼 ‘**소프트 라벨(soft label)**’ (예: Label Smoothing)일 때도 동일하게 적용됩니다.

---

## 3. 왜 함께 사용할까요?

소프트맥스와 교차 엔트로피 손실은 수학적으로 완벽한 조합을 이룹니다.

1. **목적의 일치:** 소프트맥스는 모델의 출력을 교차 엔트로피가 필요로 하는 **확률 분포($p$)** 형태로 정확히 만들어 줍니다.

2. **🌟 수학적 효율성 🌟:** 모델 학습은 손실(Loss)을 줄이기 위해 가중치를 미분하는 과정(역전파)입니다. 소프트맥스와 교차 엔트로피를 조합하여 미분하면, 소프트맥스에 포함된 지수($e^x$) 함수의 미분과 교차 엔트로피의 로그($\log$) 함수의 미분이 **수학적으로 서로 상쇄**되기 때문에 그 결과(Gradient)가 **매우 간단하게 도출됩니다.** 

    $$\text{Gradient} = p_i - t_i$$

    `(예측 확률 - 실제 정답 분포)`라는 이 직관적인 '오차' 값은 모델이 얼마나 틀렸는지를 명확히 알려주며, 이 오차를 기반으로 각 가중치를 효율적이고 안정적으로 업데이트할 수 있게 해줍니다.

---

## 요약

| **함수** | **역할** | **입력** | **출력** |
| :--- | :--- | :--- | :--- |
| **소프트맥스** | 활성화 함수 | 모델의 원시 점수(로짓) | **확률 분포**<br>(0~1 사이, 총합 1) |
| **교차 엔트로피 손실** | 손실(비용) 함수 | 예측 확률 (by 소프트맥스)<br>+ 실제 정답 분포 (원-핫) | **손실 값**<br>(얼마나 틀렸는지) |
{: style="width: 100%;"}

---

## 주의: 실제 딥러닝 프레임워크에서의 구현

실제 딥러닝 프레임워크(PyTorch, TensorFlow 등)에서는 수치 안정성과 계산 효율성을 위해 이 두 함수를 따로따로 계산하지 않습니다.

대부분의 프레임워크는 **소프트맥스와 교차 엔트로피 손실을 내부적으로 결합한 단일 함수**를 제공합니다.
* **PyTorch:** `torch.nn.CrossEntropyLoss`
* **TensorFlow:** `tf.keras.losses.CategoricalCrossentropy(from_logits=True)`

따라서 개발자가 모델을 설계할 때, 모델의 **최종 출력으로 원시 점수(로짓)를 그대로** 내보내고, 이 로짓을 위와 같은 **결합된 손실 함수에 입력**하는 것이 표준입니다. (즉, 소프트맥스 활성화 함수를 별도로 적용하지 않습니다.)

---

> _이 글의 초안은 ChatGPT, Gemini 등 AI와 주고받은 질의응답 내용을 토대로 작성되었습니다._
{: .prompt-info }