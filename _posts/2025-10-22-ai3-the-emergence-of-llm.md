---
title: "AI는 어떻게 언어를 이해하는가? (3) - 거대 언어 모델(LLM)의 등장"
author: 1st-world
date: 2025-10-22 23:30:00 +0900
last_modified_at: 2025-10-23 23:20:00 +0900
categories: [Artificial Intelligence, Machine Learning]
tags: [nlp, transformer, encoder, decoder, bert, gpt, llm]
pin: false
---

지난 [2부 - 현대 AI의 심장, 트랜스포머](/posts/ai2-transformers-the-heart-of-modern-ai/)에서는 '번역'을 위해 설계된 트랜스포머(Transformer)의 '**인코더-디코더(Encoder-Decoder)**' 구조를 자세히 살펴보았습니다. 인코더는 문맥을 '이해'하고, 디코더는 그 의미를 바탕으로 새로운 문장을 '생성'했죠.

그런데 과학자들은 곧 이런 생각을 하게 됩니다.

"꼭 번역처럼 입력과 출력이 짝지어진 데이터가 필요할까?"  
"인터넷에 널린 방대한 텍스트 그 자체를 학습시킬 수는 없을까?"

이러한 아이디어는 트랜스포머 아키텍처를 두 갈래로 진화시켰고, 마침내 **거대 언어 모델(LLM, Large Language Model)** 시대의 막을 열었습니다.

---

## 1. 트랜스포머의 두 갈래 진화: BERT와 GPT

방대한 텍스트를 학습시키기 위해, 연구자들은 트랜스포머의 인코더와 디코더를 '분리'하기 시작했습니다.

### 1-1. BERT, 문맥 이해의 스페셜리스트 (Encoder-only)

"문장을 '생성'할 필요 없이, 문장의 '의미' 자체를 깊게 이해하는 모델이 필요해."

이 접근 방식이 바로 **BERT(Bidirectional Encoder Representations from Transformers)** 형태입니다. BERT는 트랜스포머의 **인코더(Encoder) 구조만** 가져와 깊게 쌓았습니다.

* **학습 방식:** Masked Language Model
    * 문장 중간에 일부러 구멍을 뚫고(마스킹), 모델이 그 구멍에 들어갈 단어를 맞히도록 했습니다.
    * **예:** `"나는 [MASK]에 가서 [MASK]를 마셨다."` → `[MASK] = "카페", [MASK] = "커피"`

* **특징:** 양방향(Bidirectional)
    * 정답을 맞히기 위해 문장의 **앞뒤 문맥을 모두** 참고합니다. (2부에서 살펴본 '셀프 어텐션'과 동일)

* **주 용도:** 문장 분류, 질의응답(Question Answering), 개체명 인식(NER) 등
    * 문맥을 '이해'하는 것이 중요한 작업(NLU)에 특화되었습니다.

### 1-2. GPT, 문장 생성의 스페셜리스트 (Decoder-only)

"문맥을 '이해'하는 것도 좋지만, 결국 AI는 '대답'을 생성해야 해."

이 접근 방식이 바로 **GPT(Generative Pre-trained Transformer)** 형태이며, 현대 LLM의 주류가 되었습니다. GPT는 트랜스포머의 **디코더 블록 중, 인코더-디코더 어텐션(Cross-Attention) 부분을 제외하고 '마스크드 셀프 어텐션 + 피드 포워드' 구조만** 가져와 깊게 쌓았습니다.

* **학습 방식:** Causal Language Model
    * 오직 '**다음 단어 맞히기**'만 학습합니다.
    * **예:** `"나는 카페에 가서"` → 다음 단어는? `"커피"`
    * **예:** `"나는 카페에 가서 커피를"` → 다음 단어는? `"마셨다"`

* **특징:** 단방향(Uni-directional)
    * 다음 단어를 예측해야 하므로, 2부에서 살펴본 '**마스크드 셀프 어텐션(Masked Self-Attention)**'을 사용합니다. 즉, 예측할 시점의 **미래 단어를 볼 수 없습니다.**

* **주 용도:** 챗봇, 요약, 번역, 코드 생성 등
    * 무언가를 '생성'하는 작업(NLG)에 압도적인 성능을 보여줍니다. ChatGPT 등이 이 구조를 따릅니다.

---

## 2. '거대(Large)' 모델을 움직이는 법칙

GPT와 같은 'Decoder-only' 모델을 만들었지만, 초기에는 그저 "다음 단어를 잘 맞히는 모델" 정도였습니다. 하지만 모델의 크기를 키우기 시작하자, 예상치 못했던 놀라운 일들이 벌어지기 시작했습니다.

### 2-1. 스케일링 법칙(Scaling Law)

"모델을 더 크게 만들고 데이터를 더 많이 학습시키면, 성능이 계속 좋아질까?"

OpenAI 등 여러 연구소는 막대한 자본을 투입해 실험에 돌입했고, 그 결과로 2020년에 발표한 논문 "[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)"에서 '**스케일링 법칙**'을 제시합니다.

* **내용:** 모델의 성능(손실/Loss)은 **모델의 크기(파라미터 수), 데이터셋의 크기, 학습에 사용된 컴퓨팅 파워(총 연산량, FLOPs)라는 3가지 요소에 대해 예측 가능한 멱함수(Power Law) 관계를 따른다**는 것입니다.

* **의미:** "돈(컴퓨팅 파워)과 데이터만 있다면, 모델을 키우는 것만으로도 성능을 향상시킬 수 있다!"

* **결과:** 이 법칙의 발견은 여러 모델의 개발 방향에 결정적인 근거를 제시함으로써 GPT-3, Gopher 등 수천억, 수조 개의 파라미터를 가진 초거대 모델의 등장을 촉발했습니다.

### 2-2. 창발적 능력(Emergent Ability)

"양(Quantity)이 질(Quality)을 만든다."

스케일링 법칙에 따라 모델을 무작정 키우던 중, 더 놀라운 현상이 발견됩니다. 2022년에 발표된 논문 "[Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)"에서 '**창발적 능력**'을 제시한 것입니다.

* **내용:** 작은 모델에서는 거의 무작위 수준(Random-level)에 머물던 특정 능력들이, 모델 크기가 **어느 임계점(Threshold)을 넘어서는 순간 갑자기 유의미한 성능으로 급등**하는 현상입니다.

* **예시:**
    * 산술 연산(덧셈, 뺄셈)
    * 문맥 속 단어 의미 파악(다의어 처리)
    * 지시 사항 따르기(Instruction Following)

* **의미:** 모델을 크게 만드는 것은 단순히 성능을 10% → 20%로 올리는 것이 아니라, 아예 '불가능'했던 작업을 '가능'하게 만든다는 것을 의미합니다.

> **창발적 능력에 대한 최신 분석**
>
> 최근 연구에 따르면 일부 '창발적 능력'이 모델의 능력이 불연속적으로 '점프'하는 것이 아니라, 능력은 연속적으로 개선되는데 우리가 사용하는 **'평가 지표(metric)'가 비선형적이어서** 마치 갑자기 나타나는 것처럼 보이는 '착시 현상'일 수 있다는 분석도 있습니다.
>
> 하지만 이러한 현상이 착시인지 아닌지와는 상관없이, 모델의 크기가 커짐에 따라 이전에는 불가능했던 복잡한 작업이 가능해질 수 있다는 사실 자체는 분명합니다.
{: .prompt-tip }

---

## 3. LLM, 어떻게 활용할까? (In-Context Learning)

창발적 능력을 획득한 LLM은 '범용 모델(General-purpose Model)'이 되었습니다. 과거 BERT가 '문장 분류'를 위해 별도의 추가 학습(Fine-tuning)이 필요했던 것과 달리, GPT-3 같은 LLM은 **별도 학습 없이** 지시만으로 작업을 수행할 수 있게 되었습니다.

### 3-1. 제로샷(Zero-shot) 러닝

* **정의:** 모델에게 한 번도 보여주지 않은 작업을 '지시(Instruction)'만으로 수행하게 하는 방식.

* **프롬프트 예시:**
    ```
    이 문장의 감정을 긍정/부정으로 분류해: "이 영화 정말 환상적이었어!"
    ```
    (모델은 '감정 분류'를 따로 학습한 적이 없음에도 아래와 같은 답변을 생성할 수 있습니다.)
* **모델 답변:**
    ```
    긍정
    ```

### 3-2. 퓨샷(Few-shot) 러닝

* **정의:** 모델이 작업을 잘 이해할 수 있도록 몇 개의 '예시(Demonstration)'를 함께 제공하는 방식.

* **프롬프트 예시:**
    ```
    Q: '사과'를 영어로?
    A: Apple

    Q: '집'을 영어로?
    A: House

    Q: '사랑'을 영어로?
    A: 
    ```
    (모델은 위 예시들을 보고 '한국어 → 영어 번역' 작업을 수행해야 함을 이해할 수 있습니다.)
* **모델 답변:**
    ```
    Love
    ```

이러한 **'인-컨텍스트 러닝(In-Context Learning)'** 능력 덕분에, 우리는 LLM을 마치 사람에게 일을 시키듯 '프롬프트(Prompt)'만으로 활용할 수 있게 되었습니다. 이 접근법은 2020년 GPT-3 논문 "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)"에서 **Zero-shot, Few-shot Learning**이라는 이름으로 널리 알려졌습니다.

---

## 4. LLM의 의미와 한계

### LLM과 벡터 임베딩

우리는 [1부](/posts/ai1-how-does-ai-understand-language/)에서 '정적 임베딩'을, [2부](/posts/ai2-transformers-the-heart-of-modern-ai/)에서 '문맥적 임베딩'을 배웠습니다. **LLM은 이 '문맥적 임베딩'의 끝판왕**이라 할 수 있죠. 앞서 "The animal…" 예시에서 "it"이 "animal"을 참조했듯, LLM은 수백 개의 층(Layer)을 거치며 단어의 벡터를 문맥에 맞게 매우 정교하게 가공합니다.

### LLM을 가로막는 벽

앞서 언급한 스케일링 법칙과 창발적 능력을 토대로 지금까지 LLM의 성능을 높여올 수 있었지만 사실 그 방법들이 만병통치약은 아닙니다. 현재 LLM은 '벽'에 부딪히고 있습니다.

* **환각(Hallucination)**

    * 스케일링은 모델이 '다음 단어를 그럴듯하게 예측'하는 능력은 키웠지만, '사실'을 기반으로 추론하는 능력까지 보장하지는 못했습니다. 잘못된 정보를 사실인 것처럼 자신 있게 생성하는 환각 현상은 모델이 아무리 커져도 해결되지 않습니다. 오히려 더 정교한 거짓말을 할 수도 있습니다.

* **수확 체감(Diminishing Returns)**

    * 모델의 크기와 학습 비용을 2배로 늘려도 성능 향상 폭은 그에 비례하지 않습니다. 즉, 미세한 성능 향상을 위해 수백억, 수천억의 비용이 들어가는 '가성비'의 한계에 도달하고 있습니다.

* **데이터 병목(Data Bottleneck) & 최신성 부족(Knowledge Cutoff)**
    * 스케일링 법칙은 '모델'뿐만 아니라 '고품질 데이터'도 계속해서 공급된다는 전제가 필요합니다. 하지만 인터넷의 고품질 데이터는 사실상 고갈 상태에 가까워지고 있습니다. 데이터가 부족한데 모델만 커진다면 병목 현상으로 인해 유의미한 성능 향상을 기대할 수 없습니다.

    * '최신성 부족(Knowledge Cutoff)' 문제는 스케일링으로 해결되지 않고, 모델을 한 번 학습시키는 데 수백억이 든다면 매일 최신 뉴스를 반영해 재학습시키는 것은 현실적으로 불가합니다.

---

## 3부 끝

이번 포스트에서는 트랜스포머 아키텍처가 **Encoder-only(BERT)**, **Decoder-only(GPT)** 형태로 진화하고, **스케일링 법칙**과 **창발적 능력**을 만나 **LLM**이 탄생했음을 확인했습니다. 또한, LLM이 **Zero/Few-shot Learning**이라는 강력한 활용법을 제공한다는 것도 알게 되었습니다.

하지만 LLM의 성능을 높이기 위해 스케일을 늘리는 방식은 특정 시점을 넘어설 때마다 비용 효율이 급격히 저하되고, 더 많이 공급해야 할 고품질 데이터는 이미 거의 고갈되었으며, 환각과 같은 근본적인 문제를 해결할 수도 없다는 **한계에 부딪히고 있다**는 것도 알게 되었습니다.

다음 [4부 - LLM을 더 똑똑하게 만드는 RAG & RLHF](/posts/ai4-rag-rlhf-make-llm-smarter/)에서는 이러한 LLM의 한계점을 '스케일링'이 아닌 다른 방식으로 극복하려는 기술들, **RAG(검색 증강 생성)** 및 **RLHF(인간 피드백을 통한 강화 학습)** 등에 관해 알아보도록 하겠습니다.

---

> _이 글의 초안은 ChatGPT, Gemini 등 AI와 주고받은 질의응답 내용을 토대로 작성되었습니다._
{: .prompt-info }